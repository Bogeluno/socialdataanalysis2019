{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.\n",
    "\n",
    "## Formalia:\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/suneman/socialdataanalysis2019/wiki/Assignments) carefully before proceeding. This page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "_If you fail to follow these simple instructions, it will negatively impact your grade!_\n",
    "\n",
    "**Due date and time**: The assignment is due on Monday March 4th, 2019 at 23:55. Hand in your files via [`http://peergrade.io`](http://peergrade.io/).\n",
    "\n",
    "**Peergrading date and time**: _Remember that after handing in you have 1 week hours to evaluate a few assignments written by other members of the class_. Thus, the peer evaluations are due on Monday March 11th, 2019 at 23:55."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Temporal evolution of focus crimes (From Week 2)\n",
    "\n",
    "Police chief Suneman is interested in the temporal development of only a subset of categories, the so-called focus crimes. Those categories are listed below (for convenient copy-paste action). Create bar-charts displaying the year-by-year development of each of these categories across the years 2003-2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "focuscrimes = set(['WEAPON LAWS', 'PROSTITUTION', 'DRIVING UNDER THE INFLUENCE', 'ROBBERY', 'BURGLARY', 'ASSAULT', 'DRUNKENNESS', 'DRUG/NARCOTIC', 'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'VEHICLE THEFT', 'STOLEN PROPERTY', 'DISORDERLY CONDUCT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualizing geo-data (From Week 2)\n",
    "\n",
    "*Exercise*: A bit a plotting\n",
    "* Select a couple of months of data (for example, June-July 2016) for `'DRUG/NARCOTIC'` and the same couple of month for `'LARCENY/THEFT'`. **Note**: There is an issue with folium and plotting lots of points, so if you have trouble plotting simply select a shorter time period; you should be able to plot a couple of thousand points.\n",
    "    * Draw a little circle for each arrest in the category `'DRUG/NARCOTIC'` for your time period. We can call this a kind of visualization a scatter plot\n",
    "    * Draw a little circle for each arrest in the category `'LARCENY/THEFT'` for the time period.\n",
    "* Now, let's play with heatmaps. \n",
    "   * Create a heatmap of all arrests for `'DRUG/NARCOTIC'` during June-July 2016 (with the heatmap you should be able to use the full period). Play with parameters to get plots you like\n",
    "   * Create a heatmap of all arrests for `'LARCENY/THEFT'` during June-July 2016 (with the heatmap you should be able to use the full period).\n",
    "   * Comment on the differences. What can you see using the scatter-plots that you can't see using the heatmaps? And *vice versa*: what does the heatmaps help you see that's difficult to distinguish in the scatter-plots?\n",
    "   * Comment on the effect on the various parameters for the heatmaps. How do they change the picture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Some single variable plots (From Week 3)\n",
    "\n",
    "*Excercise*: Connecting the dots and recreating plots from DAOST but using our own favorite dataset.\n",
    "\n",
    "* First create a jitter-plot (that is, code up something like **Figure 2-1** from DAOST from scratch), but based on SF Police data. My hunch from inspecting the file is that the police-folks might be a little bit lazy in noting down the exact time down to the second. So choose a crime-type and a suitable time interval (somewhere between a month and 6 months depending on the crime-type) and create a jitter plot of the arrest times **during a single hour** (e.g. 13-14). \n",
    "* Now it's time for histograms. We will now create two versions of **Figure 2-2**. \n",
    "  * Pick two crime-types with different geographical patterns **and** a suitable time-interval for each (you want between 1000 and 10000 points in your histogram)\n",
    "  * Then take the latitude part of the GPS coordinates for each crime and bin the latitudes so that you have around 50 bins across the city of SF. You can use your favorite method for binning. I like `numpy.histogram` -- that gives you the counts and then you do your own plotting. \n",
    "* **Next up** is recreating **Figure 2-4** from DAOST, but with the data you used to create Figure 211. To create the kernel density plot, you can either use `gaussian_kde` from `scipy.stats` ([for an example, check out this stackoverflow post](https://stackoverflow.com/questions/4150171/how-to-create-a-density-plot-in-matplotlib)) or you can use [`seaborn.kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html).\n",
    "* Now grab 25 random timepoints from the dataset you've just plotted and create the same Figure 2-4 plot once again, but with fewer points. Does this shed light on why I think KDEs can bee misleading? \n",
    "* Create your own two versions of **Figure 2-11**, but using the GPS data you used for your version of Figure 2-2. Comment on the result. It is not easy to create this plot from scracth, bHint: Take a look at the `scipy.stats.probplot` function. \n",
    "* Now box plots. Here, I'd like you to use the box plots to visualize fluctuations of how many crimes happen per day. For the 15 focus crimes defined last week.\n",
    "  * For the full time-span of the data, calulate the **number of crimes per day** within each category for the entire duration of the data (this should result in 15 datasets, on for each focus crime).\n",
    "  * Create a box-and whiskers plot showing the mean, median, quantiles, etc for each of the 15 crime-types and show them side-by-side. There are many ways to do this, I like to use [matplotlibs's built in functionality](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.boxplot.html)\n",
    "  * What does this plot reveal that you can't see in the plots from last time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Decision tree (From Week 4)\n",
    "\n",
    "*Exercise*: Decision trees and real-world crime data\n",
    "\n",
    "The idea for this exercise is to pick two crime-types that have *different geographical patterns* and *different temporal patterns*. We can then use various variables of the real crime data as categories to build a decision tree. I'm thinking we can use\n",
    "* `DayOfWeek` (`Sunday`, ..., `Saturday`). (Note: Will need to be encodede as integer in `sklearn`)\n",
    "* `PD District` (`TENDERLOIN`, etc). (Note: Will need to be encodede as integer in `sklearn`)\n",
    "\n",
    "And we can extract a few more from the `Time` and `Date` variables\n",
    "* Hour of the day (1-24)\n",
    "* Month of the year (1-12)\n",
    "\n",
    "So your job is to **select two crime categories** that (based on your analyses from the past three weeks) have different spatio-temporal patterns. Then we are going to to build is a decision tree (or a random forest) that takes as input the four labels (Hour-of-the-day, Day-of-the-week, Month-of-the-year, and PD-District) of some crime (from one of the two categories) and then tries to predict which category that crime is from.\n",
    "\n",
    "Some notes/hints\n",
    "* It is important for your success here to create a balanced dataset, that is, **grab an equal number of examples** from each of the two crime categories. Pick categories with lots of training data. It's probably nice to have something like 10000+ examples of each category to train on. \n",
    "* Also, I recommend you grab your training data at `random` from the set of all examples, since we want crimes to be distributed equally over time.\n",
    "* A good option is the  `DecisionTreeClassifier`.\n",
    "* Since you have created a balanced dataset, the baseline performance (random guess) is 50%. How good can your classifier get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
